---
title: 'COVID STEROID 2: HTE Analysis'
output: word_document
---

2022-09-14 Update AG: changed path to the real dataset (updated to include BW/WB variables), and added coded to change order of columns (to avoid problems as columns are selected by position and the dawols90_bestworst/worstbest variables are added before the predictors in the new dataset), finally similarly changed numbers in variable selection in rpart calls (due to the added vars), changed typo in line 633

## Analysis

First the required R packages are loaded, as well as the data set. We also source a couple of functions to allow for running BART models with multiple MCMC chains (without needing multiple cores).

```{r message=FALSE}
rm(list = ls())
library(BART)
library(caret)
library(rpart)
library(rpart.plot)
source("clusterfunctions.R")

# Load data from appropriate directory (edit as needed)
dat <- read.csv2("L:/LovbeskyttetMapper/Covid-Steroid/COVID-STEROID-2/Finale analyser/Data output/Dataset subset for BART HTE project v2.csv") #AG, real data
#dat <- read.csv("./Synth_Data/synth_covid.csv") # Bryan, synthetic data # Bryan, synthetic data
```

Next we do a small amount of data cleaning/preparation.

```{r}
# Clean up data variables types and remove the small amount of missing data
dat$resp_sup <- as.factor(dat$resp_sup)
dat$dead90 <- ifelse(dat$dead90 == TRUE, 1, 0)
dat$dead90_bestworst <- ifelse(dat$dead90_bestworst == TRUE, 1, 0)
dat$dead90_worstbest <- ifelse(dat$dead90_worstbest == TRUE, 1, 0)
dat <- dat[complete.cases(dat), ]

# Print out summaries of continuous covariates pre-standardization in case
# needed for interpretation
print(mean(dat$age, na.rm = T))
print(sd(dat$age, na.rm = T))
print(mean(dat$BL9_Weight, na.rm = T))
print(sd(dat$BL9_Weight, na.rm = T))

# Standardize continuous covariates
dat$age <- (dat$age - mean(dat$age)) / sd(dat$age)
dat$BL9_Weight <- (dat$BL9_Weight - mean(dat$BL9_Weight)) / sd(dat$BL9_Weight)

# Reorder variables to avoid problems (due to dawols90_bestworst/worstbest added before the predictors)
dat <- dat[, c(1:3, 6:17, 4:5)]

# Make datasets under each counterfactual
dat1 <- dat0 <- dat
dat1$allocation <- TRUE
dat0$allocation <- FALSE



# Rename covariates so decision tree figures can be used without edits
colnames(dat)[4:13] <- c("Age", "Limits_Care", "Respiratory_Support",
                         "Interleukin", "Early_Corticosteroids", "Weight",
                         "Diabetes", "Ischemic_Heart_Disease",
                         "Pulmonary_Disease", "Immunosuppression")
```

Then we run a BART analysis focused on the binary mortality outcome.

```{r results=FALSE}
# Perform cross validation if not already done (may take >2 hours)
cvcomplete <- TRUE
cvresults_mort <- c(3, 0.25, 400)
cvresults_dawols <- c(1, 0.95, 400)

if (cvcomplete == FALSE) {
  
  # Create 10 folds of the data set for cross-validation
  set.seed(60622)
  folds <- createFolds(dat$dead90, k = 10, list = TRUE, returnTrain = FALSE)
  
  # Initialize output matrices for prediction error from each model
  cvoutput <- expand.grid(1:3, c(0.25, 0.5, 0.95), c(50, 200, 400), NA)
  colnames(cvoutput) <- c("Power", "Base", "Ntrees", "CVMSE")
  mse <- array(NA, dim = c(27, 10))

  for (hp in 1:27) {
    
    for (i in 1:10) {
          
      # BART model
      bartmod <- lbart.cluster(x.train = dat[-folds[[i]], c(1, 4:13)],
                               y.train = dat$dead90[-folds[[i]]],
                               x.test = dat[folds[[i]], c(1, 4:13)],
                               power = cvoutput$Power[hp],
                               base = cvoutput$Base[hp],
                               ntree = cvoutput$Ntrees[hp], nchains = 4)
      bartmod$yhat.test.collapse <- apply(bartmod$yhat.test, 2, rbind)
      pred <- exp(colMeans(bartmod$yhat.test.collapse)) /
              (1 + exp(colMeans(bartmod$yhat.test.collapse)))
      mse[hp, i] <- mean((dat$dead90[folds[[i]]] - pred)^2)
          
    }
    
  }


  # Calculate 10-fold CV error for each hyperparameter combination
  cvoutput$CVMSE <- rowMeans(mse)
  
  # Fit final model under hyperparameters with minimum CV error
  set.seed(60622)
  bartmod1 <-
    lbart.cluster(x.train = dat[, c(1, 4:13)], y.train = dat$dead90,
                  x.test = dat1[, c(1, 4:13)], nchains = 4,
                  power = cvoutput$Power[which.min(cvoutput$CVMSE)],
                  base = cvoutput$Base[which.min(cvoutput$CVMSE)],
                  ntree = cvoutput$Ntrees[which.min(cvoutput$CVMSE)])
  bartmod0 <-
    lbart.cluster(x.train = dat[, c(1, 4:13)], y.train = dat$dead90,
                  x.test = dat0[, c(1, 4:13)], nchains = 4,
                  power = cvoutput$Power[which.min(cvoutput$CVMSE)],
                  base = cvoutput$Base[which.min(cvoutput$CVMSE)],
                  ntree = cvoutput$Ntrees[which.min(cvoutput$CVMSE)])
  
}

if (cvcomplete == TRUE) {
  
  # Fit final model under hyperparameters with minimum CV error
  set.seed(60622)
  bartmod1 <-
    lbart.cluster(x.train = dat[, c(1, 4:13)], y.train = dat$dead90,
                  x.test = dat1[, c(1, 4:13)], nchains = 4,
                  power = cvresults_mort[1], base = cvresults_mort[2],
                  ntree = cvresults_mort[3])
  bartmod0 <-
    lbart.cluster(x.train = dat[, c(1, 4:13)], y.train = dat$dead90,
                  x.test = dat0[, c(1, 4:13)], nchains = 4,
                  power = cvresults_mort[1], base = cvresults_mort[2],
                  ntree = cvresults_mort[3])
  
}

# Collapse predictions across chains for certain calculations
bartmod1$yhat.train.collapse <- apply(bartmod1$yhat.train, 2, rbind)
bartmod1$yhat.test.collapse <- apply(bartmod1$yhat.test, 2, rbind)
bartmod0$yhat.train.collapse <- apply(bartmod0$yhat.train, 2, rbind)
bartmod0$yhat.test.collapse <- apply(bartmod0$yhat.test, 2, rbind)
```

Print the chosen hyperparameters here for future reference:

```{r}
if (cvcomplete == FALSE) {
  print(cvoutput[which.min(cvoutput$CVMSE), ])
}
```

Then conditional average treatment effects are estimated using the predictions under each counterfactual.

```{r}
dat$cate <-
  exp(colMeans(bartmod1$yhat.test.collapse)) /
    (1 + exp(colMeans(bartmod1$yhat.test.collapse))) -
  exp(colMeans(bartmod0$yhat.test.collapse)) /
    (1 + exp(colMeans(bartmod0$yhat.test.collapse)))
```

This full process is then repeated for the continuous outcome (days alive without life support by day 90).

```{r results=FALSE}
if (cvcomplete == FALSE) {

  # Initialize output for prediction error from each model
  cvoutput$CVMSE_c <- NA
  mse_c <- array(NA, dim = c(27, 10))
  
  # Perform cross validation (should take much less time than the binary outcome)
  set.seed(60622)
  for (hp in 1:27) {
    
    for (i in 1:10) {
          
      # BART model
      bartmod_c <- wbart.cluster(x.train = dat[-folds[[i]], c(1, 4:13)],
                                 y.train = dat$dawols90[-folds[[i]]],
                                 x.test = dat[folds[[i]], c(1, 4:13)],
                                 power = cvoutput$Power[hp],
                                 base = cvoutput$Base[hp],
                                 ntree = cvoutput$Ntrees[hp], nchains = 4)
      bartmod_c$yhat.test.collapse <- apply(bartmod_c$yhat.test, 2, rbind)
      pred_c <- colMeans(bartmod_c$yhat.test.collapse)
      mse_c[hp, i] <- mean((dat$dawols90[folds[[i]]] - pred_c)^2)
          
    }
    
  }
  
  # Calculate 10-fold CV error for each hyperparameter combination
  cvoutput$CVMSE_c <- rowMeans(mse_c)
  
  # Fit final models under hyperparameters with minimum CV error
  set.seed(60622)
  bartmod1_c <-
    wbart.cluster(x.train = dat[, c(1, 4:13)], y.train = dat$dawols90,
                  x.test = dat1[, c(1, 4:13)], nchains = 4,
                  power = cvoutput$Power[which.min(cvoutput$CVMSE_c)],
                  base = cvoutput$Base[which.min(cvoutput$CVMSE_c)],
                  ntree = cvoutput$Ntrees[which.min(cvoutput$CVMSE_c)])
  bartmod0_c <-
    wbart.cluster(x.train = dat[, c(1, 4:13)], y.train = dat$dawols90,
                  x.test = dat0[, c(1, 4:13)], nchains = 4,
                  power = cvoutput$Power[which.min(cvoutput$CVMSE_c)],
                  base = cvoutput$Base[which.min(cvoutput$CVMSE_c)],
                  ntree = cvoutput$Ntrees[which.min(cvoutput$CVMSE_c)])
  
}

if (cvcomplete == TRUE) {
  
  # Fit final models under hyperparameters with minimum CV error
  set.seed(60622)
  bartmod1_c <-
    wbart.cluster(x.train = dat[, c(1, 4:13)], y.train = dat$dawols90,
                  x.test = dat1[, c(1, 4:13)], nchains = 4,
                  power = cvresults_dawols[1], base = cvresults_dawols[2],
                  ntree = cvresults_dawols[3])
  bartmod0_c <-
    wbart.cluster(x.train = dat[, c(1, 4:13)], y.train = dat$dawols90,
                  x.test = dat0[, c(1, 4:13)], nchains = 4,
                  power = cvresults_dawols[1], base = cvresults_dawols[2],
                  ntree = cvresults_dawols[3])
  
}

# Collapse predictions across chains for certain calculations
bartmod1_c$yhat.train.collapse <- apply(bartmod1_c$yhat.train, 2, rbind)
bartmod1_c$yhat.test.collapse <- apply(bartmod1_c$yhat.test, 2, rbind)
bartmod0_c$yhat.train.collapse <- apply(bartmod0_c$yhat.train, 2, rbind)
bartmod0_c$yhat.test.collapse <- apply(bartmod0_c$yhat.test, 2, rbind)

# Estimate CATEs
dat$cate_c <- colMeans(bartmod1_c$yhat.test.collapse) -
              colMeans(bartmod0_c$yhat.test.collapse)
```

Print the chosen hyperparameters here for future reference:

```{r}
if (cvcomplete == FALSE) {
  print(cvoutput[which.min(cvoutput$CVMSE_c), ])
}
```

Finally, the "fit-the-fit" approach is used to find subgroups exhibiting heterogeneity of treatment effect, starting with the binary outcome. In particular, a CART model is fit with the CATE for 90-day mortality as the outcome and the covariates as possible predictors. The model is first fit under default CART hyperparameter settings.

```{r dpi=600}
# CART model for 90 day mortality with default CART hyperparameter and
# all covariates considered
cartmod <- rpart(cate ~ ., data = dat[, c(4:13, 18)], method = "anova")
rpart.plot(cartmod)
```

Now we prune the tree for interpretability using a maximum depth of 3 nodes.

```{r dpi=600}
cartmod <- rpart(cate ~ ., data = dat[, c(4:13, 18)], method = "anova",
                 maxdepth = 3)
rpart.plot(cartmod)

# Output as PDF
pdf(file = "fig2.pdf")
rpart.plot(cartmod, yesno = 2)
dev.off()
```

Next the same fit-the-fit approach is used to summarize the results for the continuous outcome, starting with a CART model using all covariates and default hyperparameter.

```{r dpi=600}
# CART model for 90 day mortality with default CART hyperparameter and
# all covariates considered
cartmod <- rpart(cate_c ~ ., data = dat[, c(4:13, 19)], method = "anova")
rpart.plot(cartmod)
```

Now we prune the tree for interpretability using a maximum depth of 3 nodes.

```{r dpi=600}
cartmod <- rpart(cate_c ~ ., data = dat[, c(4:13, 19)], method = "anova",
                 maxdepth = 3)
rpart.plot(cartmod)

# Output as PDF
pdf(file = "fig1.pdf")
rpart.plot(cartmod, yesno = 2)
dev.off()
```

## Diagnostics

Next we run standard diagnostics for each of the models. We begin with diagnostics for the continuous outcome models, which are simpler.

```{r}
# MCMC chains for parameter in models of continuous outcome
par(mfrow = c(2, 2))
plot(bartmod0_c$sigma[, 1], type = "l", ylab = "First MCMC Chain")
abline(v = 100, lwd = 2, col = "red")
plot(bartmod0_c$sigma[, 2], type = "l", ylab = "Second MCMC Chain")
abline(v = 100, lwd = 2, col = "red")
plot(bartmod0_c$sigma[, 3], type = "l", ylab = "Third MCMC Chain")
abline(v = 100, lwd = 2, col = "red")
plot(bartmod0_c$sigma[, 4], type = "l", ylab = "Fourth MCMC Chain")
abline(v = 100, lwd = 2, col = "red")

plot(bartmod1_c$sigma[, 1], type = "l", ylab = "First MCMC Chain")
abline(v = 100, lwd = 2, col = "red")
plot(bartmod1_c$sigma[, 2], type = "l", ylab = "Second MCMC Chain")
abline(v = 100, lwd = 2, col = "red")
plot(bartmod1_c$sigma[, 3], type = "l", ylab = "Third MCMC Chain")
abline(v = 100, lwd = 2, col = "red")
plot(bartmod1_c$sigma[, 4], type = "l", ylab = "Fourth MCMC Chain")
abline(v = 100, lwd = 2, col = "red")

# Posterior diagnostics of sigma:
posterior::summarise_draws(posterior::draws_array(sigma0 = bartmod0_c$sigma[101:1100, ],
                                                  sigma1 = bartmod1_c$sigma[101:1100, ],
                                                  .nchains = 4))
```

One should check that each chain has converged after burn-in (designated by the vertical red lines). In general, the chains should be converging to approximately the same value.

Next consider diagnostics for the models with the binary mortality outcome as described in Sparapani (2021). First consider the autocorrelation of the estimated response surface from BART from 10 randomly selected subjects. This may start somewhat correlated for nearby observations, but should reduce to 0 correlation for observations further apart.

```{r}
# First for bartmod0, one panel for each chain
par(mfrow = c(2, 2))

auto.corr <- acf(bartmod0$yhat.train[ , sample(1:dim(dat)[1], 10), 1],
                 plot = FALSE)
max.lag <- max(auto.corr$lag[ , 1, 1])

j <- seq(-0.5, 0.4, length.out = 10)
for (h in 1:10) {
  if (h == 1) {
    plot(1:max.lag + j[h], auto.corr$acf[1 + (1:max.lag), h, h],
         type = 'h', xlim = c(0, max.lag + 1), ylim = c(-1, 1),
         ylab = 'acf', xlab = 'lag')
  } else {
    lines(1:max.lag + j[h], auto.corr$acf[1 + (1:max.lag), h, h],
          type = 'h', col = h)
  }
}

auto.corr <- acf(bartmod0$yhat.train[ , sample(1:dim(dat)[1], 10), 2],
                 plot = FALSE)
max.lag <- max(auto.corr$lag[ , 1, 1])

j <- seq(-0.5, 0.4, length.out = 10)
for (h in 1:10) {
  if (h == 1) {
    plot(1:max.lag + j[h], auto.corr$acf[1 + (1:max.lag), h, h],
         type = 'h', xlim = c(0, max.lag + 1), ylim = c(-1, 1),
         ylab = 'acf', xlab = 'lag')
  } else {
    lines(1:max.lag + j[h], auto.corr$acf[1 + (1:max.lag), h, h],
          type = 'h', col = h)
  }
}

auto.corr <- acf(bartmod0$yhat.train[ , sample(1:dim(dat)[1], 10), 3],
                 plot = FALSE)
max.lag <- max(auto.corr$lag[ , 1, 1])

j <- seq(-0.5, 0.4, length.out = 10)
for (h in 1:10) {
  if (h == 1) {
    plot(1:max.lag + j[h], auto.corr$acf[1 + (1:max.lag), h, h],
         type = 'h', xlim = c(0, max.lag + 1), ylim = c(-1, 1),
         ylab = 'acf', xlab = 'lag')
  } else {
    lines(1:max.lag + j[h], auto.corr$acf[1 + (1:max.lag), h, h],
          type = 'h', col = h)
  }
}

auto.corr <- acf(bartmod0$yhat.train[ , sample(1:dim(dat)[1], 10), 4],
                 plot = FALSE)
max.lag <- max(auto.corr$lag[ , 1, 1])

j <- seq(-0.5, 0.4, length.out = 10)
for (h in 1:10) {
  if (h == 1) {
    plot(1:max.lag + j[h], auto.corr$acf[1 + (1:max.lag), h, h],
         type = 'h', xlim = c(0, max.lag + 1), ylim = c(-1, 1),
         ylab = 'acf', xlab = 'lag')
  } else {
    lines(1:max.lag + j[h], auto.corr$acf[1 + (1:max.lag), h, h],
          type = 'h', col = h)
  }
}

# Then for bartmod1
auto.corr <- acf(bartmod1$yhat.train[ , sample(1:dim(dat)[1], 10), 1],
                 plot = FALSE)
max.lag <- max(auto.corr$lag[ , 1, 1])

j <- seq(-0.5, 0.4, length.out = 10)
for (h in 1:10) {
  if (h == 1) {
    plot(1:max.lag + j[h], auto.corr$acf[1 + (1:max.lag), h, h],
         type = 'h', xlim = c(0, max.lag + 1), ylim = c(-1, 1),
         ylab = 'acf', xlab = 'lag')
  } else {
    lines(1:max.lag + j[h], auto.corr$acf[1 + (1:max.lag), h, h],
          type = 'h', col = h)
  }
}

auto.corr <- acf(bartmod1$yhat.train[ , sample(1:dim(dat)[1], 10), 2],
                 plot = FALSE)
max.lag <- max(auto.corr$lag[ , 1, 1])

j <- seq(-0.5, 0.4, length.out = 10)
for (h in 1:10) {
  if (h == 1) {
    plot(1:max.lag + j[h], auto.corr$acf[1 + (1:max.lag), h, h],
         type = 'h', xlim = c(0, max.lag + 1), ylim = c(-1, 1),
         ylab = 'acf', xlab = 'lag')
  } else {
    lines(1:max.lag + j[h], auto.corr$acf[1 + (1:max.lag), h, h],
          type = 'h', col = h)
  }
}

auto.corr <- acf(bartmod1$yhat.train[ , sample(1:dim(dat)[1], 10), 3],
                 plot = FALSE)
max.lag <- max(auto.corr$lag[ , 1, 1])

j <- seq(-0.5, 0.4, length.out = 10)
for (h in 1:10) {
  if (h == 1) {
    plot(1:max.lag + j[h], auto.corr$acf[1 + (1:max.lag), h, h],
         type = 'h', xlim = c(0, max.lag + 1), ylim = c(-1, 1),
         ylab = 'acf', xlab = 'lag')
  } else {
    lines(1:max.lag + j[h], auto.corr$acf[1 + (1:max.lag), h, h],
          type = 'h', col = h)
  }
}

auto.corr <- acf(bartmod1$yhat.train[ , sample(1:dim(dat)[1], 10), 4],
                 plot = FALSE)
max.lag <- max(auto.corr$lag[ , 1, 1])

j <- seq(-0.5, 0.4, length.out = 10)
for (h in 1:10) {
  if (h == 1) {
    plot(1:max.lag + j[h], auto.corr$acf[1 + (1:max.lag), h, h],
         type = 'h', xlim = c(0, max.lag + 1), ylim = c(-1, 1),
         ylab = 'acf', xlab = 'lag')
  } else {
    lines(1:max.lag + j[h], auto.corr$acf[1 + (1:max.lag), h, h],
          type = 'h', col = h)
  }
}
```

Next, we plot the Geweke Z statistics for each individual, which should be approximately distributed as a standard Normal.

```{r}
# First for bartmod0
geweke <- gewekediag(bartmod0$yhat.train.collapse)
n <- dim(dat)[1]
j <- -10^(log10(n) - 1)
plot(geweke$z, pch = '.', cex = 2, ylab = 'z', xlab = 'i',
     xlim=c(j, n), ylim=c(-5, 5))
lines(1:n, rep(-1.96, n), type='l', col=6)
lines(1:n, rep(+1.96, n), type='l', col=6)
lines(1:n, rep(-2.576, n), type='l', col=5)
lines(1:n, rep(+2.576, n), type='l', col=5)
lines(1:n, rep(-3.291, n), type='l', col=4)
lines(1:n, rep(+3.291, n), type='l', col=4)
lines(1:n, rep(-3.891, n), type='l', col=3)
lines(1:n, rep(+3.891, n), type='l', col=3)
lines(1:n, rep(-4.417, n), type='l', col=2)
lines(1:n, rep(+4.417, n), type='l', col=2)
text(c(1, 1), c(-1.96, 1.96), pos=2, cex=0.6, labels='0.95')
text(c(1, 1), c(-2.576, 2.576), pos=2, cex=0.6, labels='0.99')
text(c(1, 1), c(-3.291, 3.291), pos=2, cex=0.6, labels='0.999')
text(c(1, 1), c(-3.891, 3.891), pos=2, cex=0.6, labels='0.9999')
text(c(1, 1), c(-4.417, 4.417), pos=2, cex=0.6, labels='0.99999')

# Then for bartmod1
geweke <- gewekediag(bartmod1$yhat.train.collapse)
plot(geweke$z, pch = '.', cex = 2, ylab = 'z', xlab = 'i',
     xlim=c(j, n), ylim=c(-5, 5))
lines(1:n, rep(-1.96, n), type='l', col=6)
lines(1:n, rep(+1.96, n), type='l', col=6)
lines(1:n, rep(-2.576, n), type='l', col=5)
lines(1:n, rep(+2.576, n), type='l', col=5)
lines(1:n, rep(-3.291, n), type='l', col=4)
lines(1:n, rep(+3.291, n), type='l', col=4)
lines(1:n, rep(-3.891, n), type='l', col=3)
lines(1:n, rep(+3.891, n), type='l', col=3)
lines(1:n, rep(-4.417, n), type='l', col=2)
lines(1:n, rep(+4.417, n), type='l', col=2)
text(c(1, 1), c(-1.96, 1.96), pos=2, cex=0.6, labels='0.95')
text(c(1, 1), c(-2.576, 2.576), pos=2, cex=0.6, labels='0.99')
text(c(1, 1), c(-3.291, 3.291), pos=2, cex=0.6, labels='0.999')
text(c(1, 1), c(-3.891, 3.891), pos=2, cex=0.6, labels='0.9999')
text(c(1, 1), c(-4.417, 4.417), pos=2, cex=0.6, labels='0.99999')
```

If several points lie beyond the dark blue line or further, consider using more thinning when fitting the models.

## Additional Tables/Figures

Next we create some additional figures and a table for the paper

### Treatment effect plot

```{r}

```


### Table

## Sensitivity analysis

We conduct two simple sensitivity analyses, one assuming that all missing mortality data correspond to an alive status and one assuming that all missing mortality data correspond to a deceased status. In the former scenario, missing days without life support will be set to a random sample with replacement from the observed days without life support among those known to be alive at day 90; in the latter scenario it will be set to a random sample with replacement from the observed days without life support among those not alive at day 90. Default hyperparameters are used in these analyses.

Note that these analyses are unlikely to output the exact same trees as the originl analysis, but they should hopefully output trees which largely tell the same HTE story as the original analysis.

### Sensitivity analysis 1: Best-worst imputation

First the memory is cleared and the data set is reloaded (the cross validation output is saved for future use).

```{r message=FALSE}
rm(list = ls()[!(ls() %in% list("cvoutput", "cvcomplete",
                                "cvresults_mort", "cvresults_dawols"))])
library(BART)
library(caret)
library(rpart)
library(rpart.plot)
source("clusterfunctions.R")

# Load data from appropriate directory (edit as needed)
dat <- read.csv2("L:/LovbeskyttetMapper/Covid-Steroid/COVID-STEROID-2/Finale analyser/Data output/Dataset subset for BART HTE project v2.csv") #AG, real data
#dat <- read.csv2("~/Downloads/synth_covid.csv") # Bryan, synthetic data # Bryan, synthetic data
```

Next we do a small amount of data cleaning/preparation.

```{r}
# Clean up data variables types and use best-worst imputation
dat$resp_sup <- as.factor(dat$resp_sup)
dat$dead90_bestworst <- ifelse(dat$dead90_bestworst == TRUE, 1, 0)

# Standardize continuous covariates
dat$age <- (dat$age - mean(dat$age)) / sd(dat$age)
dat$BL9_Weight <- (dat$BL9_Weight - mean(dat$BL9_Weight)) / sd(dat$BL9_Weight)

# Change order again
dat <- dat[, c(1:3, 6:17, 4:5)]

# Make datasets under each counterfactual
dat1 <- dat0 <- dat
dat1$allocation <- TRUE
dat0$allocation <- FALSE

# Rename covariates so decision tree figures can be used without edits
colnames(dat)[4:13] <- c("Age", "Limits_Care", "Respiratory_Support",
                         "Interleukin", "Early_Corticosteroids", "Weight",
                         "Diabetes", "Ischemic_Heart_Disease",
                         "Pulmonary_Disease", "Immunosuppression")
```

Then we run a BART analysis focused on the binary mortality outcome. Note that we use the hyperparameters selected during the cross-validation procedures in the main analysis.

```{r results=FALSE}
if (cvcomplete == FALSE) {
  
  # Fit BART models, get predictions under each trt
  set.seed(60622)
  bartmod1 <-
    lbart.cluster(x.train = dat[, c(1, 4:13)], y.train = dat$dead90_bestworst,
                  x.test = dat1[, c(1, 4:13)], nchains = 4,
                  power = cvoutput$Power[which.min(cvoutput$CVMSE)],
                  base = cvoutput$Base[which.min(cvoutput$CVMSE)],
                  ntree = cvoutput$Ntrees[which.min(cvoutput$CVMSE)])
  bartmod0 <-
    lbart.cluster(x.train = dat[, c(1, 4:13)], y.train = dat$dead90_bestworst,
                  x.test = dat0[, c(1, 4:13)], nchains = 4,
                  power = cvoutput$Power[which.min(cvoutput$CVMSE)],
                  base = cvoutput$Base[which.min(cvoutput$CVMSE)],
                  ntree = cvoutput$Ntrees[which.min(cvoutput$CVMSE)])
  
}

if (cvcomplete == TRUE) {
  
  # Fit BART models, get predictions under each trt
  set.seed(60622)
  bartmod1 <-
    lbart.cluster(x.train = dat[, c(1, 4:13)], y.train = dat$dead90_bestworst,
                  x.test = dat1[, c(1, 4:13)], nchains = 4,
                  power = cvresults_mort[1], base = cvresults_mort[2],
                  ntree = cvresults_mort[3])
  bartmod0 <-
    lbart.cluster(x.train = dat[, c(1, 4:13)], y.train = dat$dead90_bestworst,
                  x.test = dat0[, c(1, 4:13)], nchains = 4,
                  power = cvresults_mort[1], base = cvresults_mort[2],
                  ntree = cvresults_mort[3])
  
}


# Collapse predictions across chains for certain calculations
bartmod1$yhat.train.collapse <- apply(bartmod1$yhat.train, 2, rbind)
bartmod1$yhat.test.collapse <- apply(bartmod1$yhat.test, 2, rbind)
bartmod0$yhat.train.collapse <- apply(bartmod0$yhat.train, 2, rbind)
bartmod0$yhat.test.collapse <- apply(bartmod0$yhat.test, 2, rbind)
```

Then conditional average treatment effects are estimated using the predictions under each counterfactual.

```{r}
dat$cate <-
  exp(colMeans(bartmod1$yhat.test.collapse)) /
    (1 + exp(colMeans(bartmod1$yhat.test.collapse))) -
  exp(colMeans(bartmod0$yhat.test.collapse)) /
    (1 + exp(colMeans(bartmod0$yhat.test.collapse)))
```

This full process is then repeated for the continuous outcome (days alive without life support by day 90).

```{r results=FALSE}
if (cvcomplete == FALSE) {
  
  # Fit BART models, get predictions under each trt
  set.seed(60622)
  bartmod1_c <-
    wbart.cluster(x.train = dat[, c(1, 4:13)],
                  y.train = dat$dawols90_bestworst,
                  x.test = dat1[, c(1, 4:13)], nchains = 4,
                  power = cvoutput$Power[which.min(cvoutput$CVMSE_c)],
                  base = cvoutput$Base[which.min(cvoutput$CVMSE_c)],
                  ntree = cvoutput$Ntrees[which.min(cvoutput$CVMSE_c)])
  bartmod0_c <-
    wbart.cluster(x.train = dat[, c(1, 4:13)],
                  y.train = dat$dawols90_bestworst,
                  x.test = dat0[, c(1, 4:13)], nchains = 4,
                  power = cvoutput$Power[which.min(cvoutput$CVMSE_c)],
                  base = cvoutput$Base[which.min(cvoutput$CVMSE_c)],
                  ntree = cvoutput$Ntrees[which.min(cvoutput$CVMSE_c)])

}

if (cvcomplete == TRUE) {
  
  # Fit BART models, get predictions under each trt
  set.seed(60622)
  bartmod1_c <-
    wbart.cluster(x.train = dat[, c(1, 4:13)],
                  y.train = dat$dawols90_bestworst,
                  x.test = dat1[, c(1, 4:13)], nchains = 4,
                  power = cvresults_dawols[1], base = cvresults_dawols[2],
                  ntree = cvresults_dawols[3])
  bartmod0_c <-
    wbart.cluster(x.train = dat[, c(1, 4:13)],
                  y.train = dat$dawols90_bestworst,
                  x.test = dat0[, c(1, 4:13)], nchains = 4,
                  power = cvresults_dawols[1], base = cvresults_dawols[2],
                  ntree = cvresults_dawols[3])

}

# Collapse predictions across chains for certain calculations
bartmod1_c$yhat.train.collapse <- apply(bartmod1_c$yhat.train, 2, rbind)
bartmod1_c$yhat.test.collapse <- apply(bartmod1_c$yhat.test, 2, rbind)
bartmod0_c$yhat.train.collapse <- apply(bartmod0_c$yhat.train, 2, rbind)
bartmod0_c$yhat.test.collapse <- apply(bartmod0_c$yhat.test, 2, rbind)

# Estimate CATEs
dat$cate_c <- colMeans(bartmod1_c$yhat.test.collapse) -
              colMeans(bartmod0_c$yhat.test.collapse)
```

Finally, the "fit-the-fit" approach is used to find subgroups exhibiting heterogeneity of treatment effect, starting with the binary outcome. In particular, a CART model is fit with the CATE for 90-day mortality as the outcome and the covariates as possible predictors. The model is first fit under default CART hyperparameter settings.

```{r dpi=600}
# CART model for 90 day mortality with default CART hyperparameter and
# all covariates considered
cartmod <- rpart(cate ~ ., data = dat[, c(4:13, 18)], method = "anova")
rpart.plot(cartmod)
```

Now we prune the tree for interpretability using a maximum depth of 3 nodes.

```{r dpi=600}
cartmod <- rpart(cate ~ ., data = dat[, c(4:13, 18)], method = "anova",
                 maxdepth = 3)
rpart.plot(cartmod)

# Output as PDF
pdf(file = "suppfig2.pdf")
rpart.plot(cartmod, yesno = 2)
dev.off()
```

Next the same fit-the-fit approach is used to summarize the results for the continuous outcome, starting with a CART model using all covariates and default hyperparameter.

```{r dpi=600}
# CART model for 90 day mortality with default CART hyperparameter and
# all covariates considered
cartmod <- rpart(cate_c ~ ., data = dat[, c(4:13, 19)], method = "anova")
rpart.plot(cartmod)
```

Now we prune the tree for interpretability using a maximum depth of 3 nodes.

```{r dpi=600}
cartmod <- rpart(cate_c ~ ., data = dat[, c(4:13, 19)], method = "anova",
                 maxdepth = 3)
rpart.plot(cartmod)

# Output as PDF
pdf(file = "suppfig1.pdf")
rpart.plot(cartmod, yesno = 2)
dev.off()
```

### Sensitivity analysis 2: Worst-best imputation

First the memory is cleared and the data set is reloaded.

```{r message=FALSE}
rm(list = ls()[!(ls() %in% list("cvoutput", "cvcomplete",
                                "cvresults_mort", "cvresults_dawols"))])
library(BART)
library(caret)
library(rpart)
library(rpart.plot)
source("clusterfunctions.R")

# Load data from appropriate directory (edit as needed)
dat <- read.csv2("L:/LovbeskyttetMapper/Covid-Steroid/COVID-STEROID-2/Finale analyser/Data output/Dataset subset for BART HTE project v2.csv") #AG, real data
#dat <- read.csv2("~/Downloads/synth_covid.csv") # Bryan, synthetic data # Bryan, synthetic data
```

Next we do a small amount of data cleaning/preparation.

```{r}
# Clean up data variables types and use worst-best imputation
dat$resp_sup <- as.factor(dat$resp_sup)
dat$dead90_worstbest <- ifelse(dat$dead90_worstbest == TRUE, 1, 0)

# Standardize continuous covariates
dat$age <- (dat$age - mean(dat$age)) / sd(dat$age)
dat$BL9_Weight <- (dat$BL9_Weight - mean(dat$BL9_Weight)) / sd(dat$BL9_Weight)

# Change order again
dat <- dat[, c(1:3, 6:17, 4:5)]

# Make datasets under each counterfactual
dat1 <- dat0 <- dat
dat1$allocation <- TRUE
dat0$allocation <- FALSE

# Rename covariates so decision tree figures can be used without edits
colnames(dat)[4:13] <- c("Age", "Limits_Care", "Respiratory_Support",
                         "Interleukin", "Early_Corticosteroids", "Weight",
                         "Diabetes", "Ischemic_Heart_Disease",
                         "Pulmonary_Disease", "Immunosuppression")
```

Then we run a BART analysis focused on the binary mortality outcome.

```{r results=FALSE}
if (cvcomplete == FALSE) {
  
  # Fit BART models, get predictions under each trt
  set.seed(60622)
  bartmod1 <-
    lbart.cluster(x.train = dat[, c(1, 4:13)], y.train = dat$dead90_worstbest,
                  x.test = dat1[, c(1, 4:13)], nchains = 4,
                  power = cvoutput$Power[which.min(cvoutput$CVMSE)],
                  base = cvoutput$Base[which.min(cvoutput$CVMSE)],
                  ntree = cvoutput$Ntrees[which.min(cvoutput$CVMSE)])
  bartmod0 <-
    lbart.cluster(x.train = dat[, c(1, 4:13)], y.train = dat$dead90_worstbest,
                  x.test = dat0[, c(1, 4:13)], nchains = 4,
                  power = cvoutput$Power[which.min(cvoutput$CVMSE)],
                  base = cvoutput$Base[which.min(cvoutput$CVMSE)],
                  ntree = cvoutput$Ntrees[which.min(cvoutput$CVMSE)])

}

if (cvcomplete == TRUE) {
  
  # Fit BART models, get predictions under each trt
  set.seed(60622)
  bartmod1 <-
    lbart.cluster(x.train = dat[, c(1, 4:13)], y.train = dat$dead90_worstbest,
                  x.test = dat1[, c(1, 4:13)], nchains = 4,
                  power = cvresults_mort[1], base = cvresults_mort[2],
                  ntree = cvresults_mort[3])
  bartmod0 <-
    lbart.cluster(x.train = dat[, c(1, 4:13)], y.train = dat$dead90_worstbest,
                  x.test = dat0[, c(1, 4:13)], nchains = 4,
                  power = cvresults_mort[1], base = cvresults_mort[2],
                  ntree = cvresults_mort[3])

}

# Collapse predictions across chains for certain calculations
bartmod1$yhat.train.collapse <- apply(bartmod1$yhat.train, 2, rbind)
bartmod1$yhat.test.collapse <- apply(bartmod1$yhat.test, 2, rbind)
bartmod0$yhat.train.collapse <- apply(bartmod0$yhat.train, 2, rbind)
bartmod0$yhat.test.collapse <- apply(bartmod0$yhat.test, 2, rbind)
```

Then conditional average treatment effects are estimated using the predictions under each counterfactual.

```{r}
dat$cate <-
  exp(colMeans(bartmod1$yhat.test.collapse)) /
    (1 + exp(colMeans(bartmod1$yhat.test.collapse))) -
  exp(colMeans(bartmod0$yhat.test.collapse)) /
    (1 + exp(colMeans(bartmod0$yhat.test.collapse)))
```

This full process is then repeated for the continuous outcome (days alive without life support by day 90).

```{r results=FALSE}
if (cvcomplete == FALSE) {
  
  # Fit BART models, get predictions under each trt
  set.seed(60622)
  bartmod1_c <-
    wbart.cluster(x.train = dat[, c(1, 4:13)],
                  y.train = dat$dawols90_worstbest,
                  x.test = dat1[, c(1, 4:13)], nchains = 4,
                  power = cvoutput$Power[which.min(cvoutput$CVMSE_c)],
                  base = cvoutput$Base[which.min(cvoutput$CVMSE_c)],
                  ntree = cvoutput$Ntrees[which.min(cvoutput$CVMSE_c)])
  bartmod0_c <-
    wbart.cluster(x.train = dat[, c(1, 4:13)],
                  y.train = dat$dawols90_worstbest,
                  x.test = dat0[, c(1, 4:13)], nchains = 4,
                  power = cvoutput$Power[which.min(cvoutput$CVMSE_c)],
                  base = cvoutput$Base[which.min(cvoutput$CVMSE_c)],
                  ntree = cvoutput$Ntrees[which.min(cvoutput$CVMSE_c)])

}

if (cvcomplete == TRUE) {
  
  # Fit BART models, get predictions under each trt
  set.seed(60622)
  bartmod1_c <-
    wbart.cluster(x.train = dat[, c(1, 4:13)],
                  y.train = dat$dawols90_worstbest,
                  x.test = dat1[, c(1, 4:13)], nchains = 4,
                  power = cvresults_dawols[1], base = cvresults_dawols[2],
                  ntree = cvresults_dawols[3])
  bartmod0_c <-
    wbart.cluster(x.train = dat[, c(1, 4:13)],
                  y.train = dat$dawols90_worstbest,
                  x.test = dat0[, c(1, 4:13)], nchains = 4,
                  power = cvresults_dawols[1], base = cvresults_dawols[2],
                  ntree = cvresults_dawols[3])

}

# Collapse predictions across chains for certain calculations
bartmod1_c$yhat.train.collapse <- apply(bartmod1_c$yhat.train, 2, rbind)
bartmod1_c$yhat.test.collapse <- apply(bartmod1_c$yhat.test, 2, rbind)
bartmod0_c$yhat.train.collapse <- apply(bartmod0_c$yhat.train, 2, rbind)
bartmod0_c$yhat.test.collapse <- apply(bartmod0_c$yhat.test, 2, rbind)

# Estimate CATEs
dat$cate_c <- colMeans(bartmod1_c$yhat.test.collapse) -
              colMeans(bartmod0_c$yhat.test.collapse)
```

Finally, the "fit-the-fit" approach is used to find subgroups exhibiting heterogeneity of treatment effect, starting with the binary outcome. In particular, a CART model is fit with the CATE for 90-day mortality as the outcome and the covariates as possible predictors. The model is first fit under default CART hyperparameter settings.

```{r dpi=600}
# CART model for 90 day mortality with default CART hyperparameter and
# all covariates considered
cartmod <- rpart(cate ~ ., data = dat[, c(4:13, 18)], method = "anova")
rpart.plot(cartmod)
```

Now we prune the tree for interpretability using a maximum depth of 3 nodes.

```{r dpi=600}
cartmod <- rpart(cate ~ ., data = dat[, c(4:13, 18)], method = "anova",
                 maxdepth = 3)
rpart.plot(cartmod)

# Output as PDF
pdf(file = "suppfig4.pdf")
rpart.plot(cartmod, yesno = 2)
dev.off()
```

Next the same fit-the-fit approach is used to summarize the results for the continuous outcome, starting with a CART model using all covariates and default hyperparameter.

```{r dpi=600}
# CART model for 90 day mortality with default CART hyperparameter and
# all covariates considered
cartmod <- rpart(cate_c ~ ., data = dat[, c(4:13, 19)], method = "anova")
rpart.plot(cartmod)
```

Now we prune the tree for interpretability using a maximum depth of 3 nodes.

```{r dpi=600}
cartmod <- rpart(cate_c ~ ., data = dat[, c(4:13, 19)], method = "anova",
                 maxdepth = 3)
rpart.plot(cartmod)

# Output as PDF
pdf(file = "suppfig3.pdf")
rpart.plot(cartmod, yesno = 2)
dev.off()
```

AG: Log date and session info (reproducibility/save package versions etc.):
```{r}
date()
sessionInfo()
```
